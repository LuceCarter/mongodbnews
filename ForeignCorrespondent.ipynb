{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the articles data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Third-party\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, ConnectionFailure\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>artDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Posle nesreće izazvane obrušavanjem nadstrešn...</td>\n",
       "      <td>UKCV: Nema novih žrtava, troje povređenih i da...</td>\n",
       "      <td>2024-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Urušavanje nadstrešnice Železničke stanice u ...</td>\n",
       "      <td>Arhitekta: Urušavanje dela Železničke stanice ...</td>\n",
       "      <td>2024-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Centar za lokalnu samoupravu (CLS) saopštio j...</td>\n",
       "      <td>CLS: JKP Gradska čistoća duguje budžetu Beogra...</td>\n",
       "      <td>2024-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zvaničnici Evropske unije i više država, među...</td>\n",
       "      <td>Zvaničnici EU i više država izrazili saučešće ...</td>\n",
       "      <td>2024-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pravni fakultet u Beogradu saopštio je da inf...</td>\n",
       "      <td>Pravni fakultet: Koleginica nagazila na utični...</td>\n",
       "      <td>2024-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0   Posle nesreće izazvane obrušavanjem nadstrešn...   \n",
       "1   Urušavanje nadstrešnice Železničke stanice u ...   \n",
       "2   Centar za lokalnu samoupravu (CLS) saopštio j...   \n",
       "3   Zvaničnici Evropske unije i više država, među...   \n",
       "4   Pravni fakultet u Beogradu saopštio je da inf...   \n",
       "\n",
       "                                               title     artDate  \n",
       "0  UKCV: Nema novih žrtava, troje povređenih i da...  2024-11-02  \n",
       "1  Arhitekta: Urušavanje dela Železničke stanice ...  2024-11-01  \n",
       "2  CLS: JKP Gradska čistoća duguje budžetu Beogra...  2024-11-01  \n",
       "3  Zvaničnici EU i više država izrazili saučešće ...  2024-11-01  \n",
       "4  Pravni fakultet: Koleginica nagazila na utični...  2024-10-31  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('fa_articles.csv', encoding='utf-8')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the embeddings for Serbian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: djovak/embedic-large\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('djovak/embedic-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function for concatenating the article's title and content and passing to the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate embeddings for concatenated title and content using Embedić.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'title' and 'content' columns\n",
    "        batch_size: Number of texts to process at once\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'embedding' column\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_emb = df.copy()\n",
    "    \n",
    "    # Concatenate title and content\n",
    "    print(\"Concatenating title and content...\")\n",
    "    df_emb['text_for_embedding'] = df_emb['title'] + \" \" + df_emb['content']\n",
    "    \n",
    "    # Generate embeddings in batches\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    \n",
    "    # Convert texts to list for batch processing\n",
    "    texts = df_emb['text_for_embedding'].tolist()\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_text = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch_text)\n",
    "        embeddings.extend(batch_embeddings.tolist())\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df_emb['embedding'] = embeddings\n",
    "    \n",
    "    # Remove temporary concatenated text column\n",
    "    df_emb = df_emb.drop('text_for_embedding', axis=1)\n",
    "    \n",
    "    print(f\"Generated embeddings for {len(df_emb)} articles\")\n",
    "    print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "    \n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating title and content...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4f8f9e38594435881b43ecb12ec17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dbd52f37ef479ea90870d72c51519a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab491c63959a4649ab2fdfbf0e6c0b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf5a7f99c19417e830e7c8930221fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f93a28f782347e7a66bf20ca3a53355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbab36577734d4aa9f300f5f2345882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d018490753f2421d84d1912b1b5c8c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd4a1da98304f599fd06c97473b366e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7835f892298e47e9a3013993a6026a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2340fa27ef8b4083847bb412e5894698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85798eb11954b199bcbb394823bfac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325271c5ecf4419cb762c173ab501033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e253d5358a64af7a9ae243a6cb4d0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dbc6d16eeb4eeca8cafdee1a11e1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f186b2d5b941c4ad4a11a5ff93178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0212a75d0f164a3cb2e3b14e82cc3edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bfde8f049e476ca209e9901301ab44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968a61164c444a00b0ad3bbf2086d45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c433ba91a8504ee699c0fc21355c0213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c9c32915c4b269e7e51f60899a214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a7bc6f19d74e93b85e61533612ef4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc46d059410f485f84faed8e6bc8a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab76466f47d4c349ab67a0f9a88a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd922c7fd6e447c96e4141628c026e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7521343cfb41403690e52d418b35d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0be3ea2e134b8c81a64dc3949d83a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd03ac2a2939416bbbe5b135c868f2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1cfb20b567437297b570d8253f6429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115706c8f30b493b8234db3c93f3f888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 873 articles\n",
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# shorten the time - uncomment but the results will be much less meaningful!\n",
    "# articles = articles.sample(20)\n",
    "\n",
    "df_with_embeddings = generate_embeddings(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the generated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame shape: (873, 4)\n",
      "\n",
      "Columns: ['content', 'title', 'artDate', 'embedding']\n",
      "\n",
      "Sample embedding (first 5 dimensions):\n",
      "[0.036100856959819794, -0.03139951825141907, -0.02158968150615692, -0.02952212281525135, 0.06143483147025108]\n",
      "\n",
      "All embeddings have same dimension: True\n",
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Preview the results\n",
    "print(\"\\nDataFrame shape:\", df_with_embeddings.shape)\n",
    "print(\"\\nColumns:\", df_with_embeddings.columns.tolist())\n",
    "\n",
    "# Sample embedding vector (first 5 dimensions)\n",
    "print(\"\\nSample embedding (first 5 dimensions):\")\n",
    "print(df_with_embeddings['embedding'].iloc[0][:5])\n",
    "\n",
    "# Verify embedding dimensions are consistent\n",
    "embedding_lengths = df_with_embeddings['embedding'].apply(len)\n",
    "print(\"\\nAll embeddings have same dimension:\", embedding_lengths.nunique() == 1)\n",
    "print(\"Embedding dimension:\", embedding_lengths.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_mongodb(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert DataFrame rows to MongoDB documents.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with embeddings\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries ready for MongoDB insertion\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        document = {           \n",
    "            'title': row['title'],\n",
    "            'content': row['content'],\n",
    "            'date': row['artDate'],  # Assuming you have a date column\n",
    "            'embedding': row['embedding'],\n",
    "            'metadata': {\n",
    "                'embedding_model': 'djovak/embedic-large',\n",
    "                'created_at': datetime.now(),\n",
    "                'last_updated': datetime.now()\n",
    "            }\n",
    "        }\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186141c940da430fa4a6e4cc6dc4ce23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 873 documents for MongoDB insertion\n",
      "\n",
      "Sample document structure (excluding embedding vector):\n",
      "{\n",
      "  \"title\": \"UKCV: Nema novih \\u017ertava, troje povre\\u0111enih i dalje u te\\u0161kom stanju\",\n",
      "  \"content\": \" Posle nesre\\u0107e izazvane obru\\u0161avanjem nadstre\\u0161nice na \\u017delezni\\u010dkoj stanici u Novom Sadu, novih \\u017ertava nema, a troje povre\\u0111enih je u te\\u0161kom op\\u0161tem stanju, potvr\\u0111eno je za Tanjug u Univerzitetskom klini\\u010dkom centru Vojvodine (UKCV). \\u201eTroje povre\\u0111enih koji su ju\\u010de primljeni u UKCV i dalje su u te\\u0161kom stanju, na intenzivnoj nezi\\u201c, re\\u010deno je iz Pres-slu\\u017ebe Klini\\u010dkog centra za Tanjug. Portparolka De\\u010dje bolnice Andrea \\u0110ureti\\u0107 rekla je da ni sino\\u0107 u tu bolnicu nisu primljene dve devoj\\u010dice. Povezane vesti U Srbiji danas Dan \\u017ealosti, u Novom Sadu trodnevna Vesti 08:16 27 Vesti 08:16 27 Zavr\\u0161ena akcija spasavanja: 14 stradalih i troje te\\u0161ko povre\\u0111enih u uru\\u0161avanju nadstre\\u0161nice na \\u017delezni\\u010dkoj stanici Vesti 23:31 602 Vesti 23:31 602 \\u201ePovodom informacije koju prenose mediji da su dve devoj\\u010dice preminule, mi ne znamo da li su devoj\\u010dice bile na \\u017delezni\\u010dkoj stanici i mo\\u017eda su me\\u0111u 14 poginulih, ali u bolnicu nisu primljene i nema novih \\u017ertvi\\u201c, re\\u010deno je iz De\\u010dje bolnice u Novom Sadu. Za dalje informacije za javnost, nadle\\u017ena je policija. Mediji prenose da su tokom no\\u0107i dve devoj\\u010dice preminule u Urgentnom centru u Novom Sadu. Prema poslednjim zvani\\u010dnim podacima, \\u010detrnaestoro ljudi je ju\\u010de poginulo, a troje je povre\\u0111eno u nesre\\u0107i izazvanoj uru\\u0161avanjem nadstre\\u0161nice na staroj zgradi \\u017delezni\\u010dke stanice u Novom Sadu. Vlada Srbije proglasila je 2. novembar za Dan \\u017ealosti zbog ove tragedije, dok je u Novom Sadu progla\\u0161ena trodnevna \\u017ealost.\",\n",
      "  \"date\": \"2024-11-02\",\n",
      "  \"embedding\": \"<embedding vector with 1024 dimensions>\",\n",
      "  \"metadata\": {\n",
      "    \"embedding_model\": \"djovak/embedic-large\",\n",
      "    \"created_at\": \"2024-12-02 22:59:53.622097\",\n",
      "    \"last_updated\": \"2024-12-02 22:59:53.622097\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to MongoDB documents\n",
    "mongodb_documents = prepare_for_mongodb(df_with_embeddings)\n",
    "print(f\"\\nPrepared {len(mongodb_documents)} documents for MongoDB insertion\")\n",
    "print(\"\\nSample document structure (excluding embedding vector):\")\n",
    "sample_doc = mongodb_documents[0].copy()\n",
    "sample_doc['embedding'] = f\"<embedding vector with {len(sample_doc['embedding'])} dimensions>\"\n",
    "print(json.dumps(sample_doc, default=str, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data into the MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB connection setup\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "DB_NAME = os.getenv(\"DB_NAME_1\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\")\n",
    "\n",
    "if not all([MONGODB_URI, DB_NAME, COLLECTION_NAME]):\n",
    "    raise ValueError(\"Missing required environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MongoDB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['cluster0-shard-00-00.5rzn4.mongodb.net:27017', 'cluster0-shard-00-01.5rzn4.mongodb.net:27017', 'cluster0-shard-00-02.5rzn4.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-zxkpmf-shard-0', tls=True) Database(MongoClient(host=['cluster0-shard-00-00.5rzn4.mongodb.net:27017', 'cluster0-shard-00-01.5rzn4.mongodb.net:27017', 'cluster0-shard-00-02.5rzn4.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-zxkpmf-shard-0', tls=True), 'rag_articles') Collection(Database(MongoClient(host=['cluster0-shard-00-00.5rzn4.mongodb.net:27017', 'cluster0-shard-00-01.5rzn4.mongodb.net:27017', 'cluster0-shard-00-02.5rzn4.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-zxkpmf-shard-0', tls=True), 'rag_articles'), 'articles')\n"
     ]
    }
   ],
   "source": [
    "def get_mongodb_connection():\n",
    "    \"\"\"\n",
    "    Create and return MongoDB client, database and collection objects.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (MongoClient, Database, Collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = MongoClient(MONGODB_URI)\n",
    "        # Test connection\n",
    "        client.admin.command('ping')\n",
    "        logger.info(\"Successfully connected to MongoDB\")\n",
    "        \n",
    "        db = client[DB_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        return client, db, collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to MongoDB. Error: {str(e)}\")\n",
    "        raise ConnectionFailure(f\"MongoDB connection failed: {str(e)}\")\n",
    "\n",
    "client, db, collection = get_mongodb_connection()\n",
    "print(client, db, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for inserting data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_documents_to_mongodb(documents: List[Dict], \n",
    "                              batch_size: int = 1000) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Insert documents into MongoDB in batches.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to insert\n",
    "        batch_size: Number of documents to insert in each batch\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (successful insertions, failed insertions)\n",
    "    \"\"\"\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    print(f\"Starting insertion of {len(documents)} documents...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        try:\n",
    "            # Insert batch with ordered=False for better performance\n",
    "            result = collection.insert_many(batch, ordered=False)\n",
    "            successful += len(result.inserted_ids)\n",
    "        except BulkWriteError as e:\n",
    "            # Handle partial failures in batch\n",
    "            successful += e.details['nInserted']\n",
    "            failed += len(batch) - e.details['nInserted']\n",
    "            print(f\"Batch {i//batch_size + 1} had {e.details['nInserted']} successful and \"\n",
    "                  f\"{len(batch) - e.details['nInserted']} failed insertions\")\n",
    "    \n",
    "    return successful, failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the collection and inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting insertion of 873 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abb2b948e534edca8c61db03c77eca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insertion complete:\n",
      "Successfully inserted: 873 documents\n",
      "Failed insertions: 0 documents\n"
     ]
    }
   ],
   "source": [
    "collection.delete_many({})\n",
    "# Insert documents\n",
    "successful, failed = insert_documents_to_mongodb(mongodb_documents)\n",
    "print(f\"\\nInsertion complete:\")\n",
    "print(f\"Successfully inserted: {successful} documents\")\n",
    "print(f\"Failed insertions: {failed} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'numDimensions': 1024,\n",
       "   'path': 'embedding',\n",
       "   'similarity': 'cosine',\n",
       "   'type': 'vector'}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"numDimensions\": 1024,\n",
    "      \"path\": \"embedding\",\n",
    "      \"similarity\": \"cosine\",\n",
    "      \"type\": \"vector\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(client, database, collection, index_name=\"vector_index\"):\n",
    "    \"\"\"Create vector search index in MongoDB collection.\"\"\"\n",
    "    index_config = {\n",
    "        \"name\": index_name,\n",
    "        \"definition\": {\n",
    "            \"mappings\": {\n",
    "                \"dynamic\": True,\n",
    "                \"fields\": {\n",
    "                    \"embedding\": {\n",
    "                        \"dimensions\": 1024,\n",
    "                        \"similarity\": \"cosine\", \n",
    "                        \"type\": \"knnVector\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        collection.create_search_index(index_config)\n",
    "        print(f\"Created vector index '{index_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MongoDB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector index 'vector_index'\n"
     ]
    }
   ],
   "source": [
    "client, db, collection = get_mongodb_connection()\n",
    "create_vector_index(client, db, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection Information:\n",
      "Total documents: 873\n",
      "Indexes: {'_id_': {'v': 2, 'key': [('_id', 1)]}}\n"
     ]
    }
   ],
   "source": [
    "# Create standard indexes for better query performance\n",
    "\n",
    "#collection.create_index(\"articleDate\")\n",
    "#collection.create_index([(\"title\", \"text\"), (\"content\", \"text\")])\n",
    "\n",
    "# Verify the setup\n",
    "print(\"\\nCollection Information:\")\n",
    "print(f\"Total documents: {collection.count_documents({})}\")\n",
    "print(f\"Indexes: {collection.index_information()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51850a337004acc9f01b86850b19013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Novi Sad\n",
      "================================================================================\n",
      "\n",
      "1. Sutra otvaranje 24. Salona arhitekture Novi Sad\n",
      "Score: 0.853\n",
      "\n",
      "Excerpt: Micki/Wikimedia Commons Novosadski Salon arhitekture biće svečano otvoren u subotu, 26. oktobra, u 19 sati u Muzeju savremene umetnosti Vojvodine. Salon arhitekture Novi Sad je međunarodna smotra aktu...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Građani odaju poštu stradalima: „Ovakva katastrofa nije zadesila Novi Sad od bombardovanja“\n",
      "Score: 0.847\n",
      "\n",
      "Excerpt: U blizini Železničke stanice u Novom Sadu pristižu građani kako bi odali poštu stradalima u nesreći. „Došao sam da odem poštu stradalima. Juče sam bio na stanici sa mojim bratancem, šetali smo. Zaprep...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. U Novom Sadu potpuni muk: „Ovakav zločin, a ne nesreća, ne sme ostati zaboravljen“\n",
      "Score: 0.844\n",
      "\n",
      "Excerpt: U centru Novom Sadu okupio se veliki broj građana koji pale sveće, javlja reporteka . Među okupljenima vlada potpuni muk. Poziv na okupljanje, nešto ranije, uputili su iz neformlane grupe „Stav“. Mila...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Novosadski novinar: Procedura nema, ugovori tajni, ofrlje urađeni poslovi kojim rukovode vlasnici pečenjara\n",
      "Score: 0.842\n",
      "\n",
      "Excerpt: Novinar Igor Mihaljević zatražio je da neko Novosađanima i Novosađankama objasni kako je moguće da su svi tenderi plaćeni novcem građana odjednom državna tajna. „Kada svi znamo da je Kini u interesu d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Novaković : Za tragediju u Novom Sadu krivi Ministarstvo infrastrukture, Železnice Srbije i Gradska uprava, odgovorni da podnesu ostavke\n",
      "Score: 0.841\n",
      "\n",
      "Excerpt: Sanja Kosović/ \"Za tregediju u Novom Sadu svi nadležni moraju da preuzmu odgovornost - od Ministarstva infrastrukture, Javnog preduzeća Železnice Srbije, do Gradske uprave... Umesto da sami preuzmu od...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "6. Građani i u Podgorici pale sveće za stradale u Novom Sadu\n",
      "Score: 0.841\n",
      "\n",
      "Excerpt: TANJUG/ NENAD MIHAJLOVIĆ Ispred Ambasade Srbije u Podgorici večeras je organizovano paljenje sveća za stradale u nesreći u Novom Sadu u kojoj je život izgubilo 14 ljudi, nakon pada nadstrešnice na Žel...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a8cfeb65b240319ad80ea96677eaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Lithium\n",
      "================================================================================\n",
      "\n",
      "1. Miki Krstović o litijumu: Ukoliko je zlo, siguran sam da narod nije toliko lud da to dozvoli\n",
      "Score: 0.848\n",
      "\n",
      "Excerpt: Glumac Miodrag Krstović, gostujući u \"360 stepeni\", rekao je da o litijumu \"treba da odlučuje struka, a ne glumci ili političari\", kao i da je siguran, ukoliko je iskopavanje zlo - \"da narod nije toli...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Vučković: Proširenje ponovo u fokusu EU, Srbija treba da proceni da li to može da iskoristi\n",
      "Score: 0.839\n",
      "\n",
      "Excerpt: „U ovom geopolitičkom trenutku EU je ponovo među svoje prioritete stavila proširenje. Hajde da vidimo da li to možemo da iskoristimo za ono što je naš cilj, a to je da postanemo član tog kluba“, izjav...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Kostić (Dveri): Apsolutno smo protiv rudarenja litijuma, ostajemo čvrsta opozicija\n",
      "Score: 0.836\n",
      "\n",
      "Excerpt:  Predsednik Srpskog pokreta Dveri Ivan Kostić izjavio je da je ta stranka \"apsolutno protiv rudarenja litijuma i bora i podržava sve ekološke predstavnike iz ekoloških organizacija, kao što je 'Ne dam...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Protest protiv litijuma u Valjevu 1. novembra – biće saopštene dalje akcije\n",
      "Score: 0.833\n",
      "\n",
      "Excerpt:  Advokat Branko Ivković iz Valjevskog pokreta otpora saopštio je danas da će 1. novembra u tom gradu biti organizovane demonstracije zbog namere vlasti da odobri otvaranje rudnika litijuma. Ivković je...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Majstorović: Litijum nije uslov, ali ne treba biti naivan – pomoglo bi Srbiji na putu ka EU\n",
      "Score: 0.832\n",
      "\n",
      "Excerpt: \"Ne treba biti naivan pa pomisliti da saradnja u oblasti obezbeđivanja litijuma za evropsku autoindustriju ne bi značila podršku vladi ove zemlje a onda i evropskim integracijama Srbije\", kaže za  Srđ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "6. Valjevski pokret otpora pozvao kompaniju Euro Litijum Balkan da napusti grad do 1. novembra\n",
      "Score: 0.828\n",
      "\n",
      "Excerpt:  Konferencija za novinare Valjevskog pokreta otpora (VPO) bila je posvećena borbi protiv delovanja kanadske kompanije Euro Litijum Balkan (ELB) u valjevskoj opštini. Branko Ivković iz VPO je pozvao Va...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(collection, query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using Embedić vector similarity.\n",
    "    \n",
    "    Args:\n",
    "        collection: MongoDB collection\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of matching documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate embedding for query using Embedić\n",
    "        query_embedding = model.encode(query).tolist()\n",
    "        \n",
    "        # Perform vector search\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": \"vector_index\",\n",
    "                    \"queryVector\": query_embedding,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"numCandidates\": 100,\n",
    "                    \"limit\": k\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"title\": 1,\n",
    "                    \"content\": 1,\n",
    "                    \"date\": 1,             \n",
    "                    \"score\": { \"$meta\": \"vectorSearchScore\" }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def display_search_results(query: str, results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Display search results in a readable format\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {doc['title']}\")\n",
    "        print(f\"Score: {doc['score']:.3f}\")\n",
    "        print(f\"\\nExcerpt: {doc['content'][:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Test the search\n",
    "queries = [\n",
    "    \"Novi Sad\",\"Lithium\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = semantic_search(collection, query, k=6)\n",
    "    display_search_results(query, results)\n",
    "    print(\"\\n\" + \"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\desktop\\correspondent\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dell\\desktop\\correspondent\\venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\desktop\\correspondent\\venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\desktop\\correspondent\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\desktop\\correspondent\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def analyze_article_topics(\n",
    "    collection,\n",
    "    min_k: int = 2,\n",
    "    max_k: int = 10,\n",
    "    embedding_field: str = \"embedding\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze topics in articles stored in MongoDB using their embeddings\n",
    "    \n",
    "    Args:\n",
    "        collection: MongoDB collection object\n",
    "        min_k: Minimum number of clusters to try\n",
    "        max_k: Maximum number of clusters to try\n",
    "        embedding_field: Name of the field containing embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Fetch all documents\n",
    "    documents = list(collection.find(\n",
    "        {embedding_field: {\"$exists\": True}},\n",
    "        {\"title\": 1, \"content\": 1, \"date\": 1, embedding_field: 1}\n",
    "    ))\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents found with embeddings\")\n",
    "    \n",
    "    # Extract embeddings and create a mapping of texts\n",
    "    embeddings = np.array([doc[embedding_field] for doc in documents])\n",
    "    \n",
    "    # Create document summaries for easier reference\n",
    "    doc_summaries = [\n",
    "        {\n",
    "            \"id\": str(doc[\"_id\"]),\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"date\": doc[\"date\"],\n",
    "            \"preview\": doc[\"content\"][:200] + \"...\"  # First 200 chars\n",
    "        }\n",
    "        for doc in documents\n",
    "    ]\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    optimal_k, scores = get_optimal_k(embeddings, k_range=range(min_k, max_k + 1))\n",
    "    \n",
    "    # Perform clustering with optimal k\n",
    "    labels, kmeans = cluster_documents(embeddings, k=optimal_k)\n",
    "    \n",
    "    # Get representative documents for each cluster\n",
    "    central_docs = find_central_documents(embeddings, labels, doc_summaries)\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    cluster_stats = calculate_cluster_stats(doc_summaries, labels)\n",
    "    \n",
    "    # Organize results\n",
    "    results = {\n",
    "        \"optimal_k\": optimal_k,\n",
    "        \"silhouette_scores\": scores,\n",
    "        \"cluster_assignments\": [int(label) for label in labels],\n",
    "        \"cluster_stats\": cluster_stats,\n",
    "        \"representative_documents\": central_docs,\n",
    "        \"document_mapping\": {\n",
    "            str(doc[\"_id\"]): {\n",
    "                \"cluster\": int(label),\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"date\": doc[\"date\"].isoformat() if isinstance(doc[\"date\"], datetime) else doc[\"date\"]\n",
    "            }\n",
    "            for doc, label in zip(documents, labels)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Keep the helper functions from before\n",
    "def calculate_cluster_stats(doc_summaries: List[Dict], labels: np.ndarray) -> List[Dict]:\n",
    "    \n",
    "    cluster_stats = []\n",
    "    unique_labels = sorted(set(labels))\n",
    "    \n",
    "    for cluster_id in unique_labels:\n",
    "        cluster_mask = labels == cluster_id\n",
    "        cluster_docs = [doc for doc, is_in_cluster in zip(doc_summaries, cluster_mask) if is_in_cluster]\n",
    "        \n",
    "        dates = [\n",
    "            datetime.fromisoformat(doc[\"date\"]) if isinstance(doc[\"date\"], str) \n",
    "            else doc[\"date\"] \n",
    "            for doc in cluster_docs\n",
    "        ]\n",
    "        \n",
    "        stats = {\n",
    "            \"cluster_id\": int(cluster_id),\n",
    "            \"size\": int(sum(cluster_mask)),\n",
    "            \"earliest_date\": min(dates).isoformat(),\n",
    "            \"latest_date\": max(dates).isoformat(),\n",
    "            \"date_range_days\": (max(dates) - min(dates)).days,\n",
    "            \"sample_titles\": [doc[\"title\"] for doc in cluster_docs[:5]]\n",
    "        }\n",
    "        cluster_stats.append(stats)\n",
    "    \n",
    "    return cluster_stats\n",
    "\n",
    "def cluster_documents(embeddings, k, random_state=42):\n",
    "   \n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return labels, kmeans\n",
    "\n",
    "def find_central_documents(embeddings, labels, doc_summaries, n_per_cluster=3):\n",
    "   \n",
    "    central_docs = {}\n",
    "    \n",
    "    for cluster_id in np.unique(labels):\n",
    "        cluster_mask = labels == cluster_id\n",
    "        cluster_embeddings = embeddings[cluster_mask]\n",
    "        cluster_docs = np.array(doc_summaries)[cluster_mask]\n",
    "        \n",
    "        centroid = cluster_embeddings.mean(axis=0)\n",
    "        distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
    "        closest_indices = np.argsort(distances)[:n_per_cluster]\n",
    "        \n",
    "        central_docs[int(cluster_id)] = cluster_docs[closest_indices].tolist()\n",
    "    \n",
    "    return central_docs\n",
    "\n",
    "def get_optimal_k(embeddings, k_range=range(2, 11)):\n",
    "    \"\"\"Previous implementation\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        scores[k] = score\n",
    "        \n",
    "    optimal_k = max(scores.items(), key=lambda x: x[1])[0]\n",
    "    return optimal_k, {int(k): float(score) for k, score in scores.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information about the discovered clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MongoDB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 6\n",
      "\n",
      "Cluster 0:\n",
      "Size: 130 articles\n",
      "Date range: 2024-10-24T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- Pravni fakultet: Koleginica nagazila na utičnicu, upala neznatno u pod, komad maltera pao na drugu\n",
      "- Aktivisti kod Starog savskog mosta: Spremni smo na sve, ovo je borba za Beograd, odbranićemo most\n",
      "- Vladimir Pajić: Stari savski most je simbol borbe za normalnu i poštenu Srbiju\n",
      "- U Sarajevu naređena evakuacija zbog mogućeg klizišta, meštani odbili\n",
      "- Inicijativa „Most ostaje“ pozvala na okupljanje i paljenje sveća kod Savskog mosta\n",
      "\n",
      "Cluster 1:\n",
      "Size: 112 articles\n",
      "Date range: 2024-10-24T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- Bećirović u SB UN optužio Srbiju da želi da odvoji RS od BiH i pripoji je sebi\n",
      "- DSK: Izveštaj o napretku govori o nesposobnosti vlade na čijem je čelu Kurti\n",
      "- Lopandić: Predstavnici Zapada „presrećni“ što Aleksandar Vučić nije prisustvovao samitu BRIKS-a\n",
      "- Savet bezbednosti UN produžio mandat EUFOR-a u BiH za još godinu dana\n",
      "- Đurđević Stamenkovski poručila Briselu: Vaše kritike su nedobronamerne, Kosovo je deo Srbije\n",
      "\n",
      "Cluster 2:\n",
      "Size: 153 articles\n",
      "Date range: 2024-10-24T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- Uhapšen švajcarski državljanin zbog pokazivanja simbola Velike Albanije u Beogradu\n",
      "- Međunarodna poternica za okorelim ubicom u Crnoj Gori: Uključene i policije Srbije i Crne Gore (VIDEO)\n",
      "- Andrej Gnjot na slobodi, napustio Srbiju: Nalazi se na teritoriji EU\n",
      "- Izbušene gume na automobilima srpskih registracija u Kosovskoj Mitrovici\n",
      "- Uhapšen osumnjičeni za pokušaj ubistva u Nišu\n",
      "\n",
      "Cluster 3:\n",
      "Size: 163 articles\n",
      "Date range: 2024-10-24T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- CLS: JKP Gradska čistoća duguje budžetu Beograda 1,55 milijardi dinara\n",
      "- Država traži kupce za svoj kapital u saobraćajnom preduzeću Lastra, poziva ih da predlože cenu\n",
      "- Od danas veći roditeljski dodatak za decu rođenu od početka godine\n",
      "- Nove cene goriva: Dobra vest za vozače\n",
      "- EPS ugasio 106 blagajni za naplatu električne energije\n",
      "\n",
      "Cluster 4:\n",
      "Size: 229 articles\n",
      "Date range: 2024-10-24T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- Pola veka „orlovog“ leta: Piloti sa emocijama o bombarderu J-22, „modernizacija nema smisla“\n",
      "- Ministarstvo pravde će precizirati članove Krivičnog zakonika na koje su stigle primedbe\n",
      "- Jerinić: U izmenama Krivičnog zakonika tri grupe problematičnih rešenja\n",
      "- Istraživanje Galupa: Građani Srbije najveći jugonostalgičari od svih stanovnika bivše SFRJ\n",
      "- (FOTO) Kostimi Hajdi Klum za Noć veštica su uvek epski\n",
      "\n",
      "Cluster 5:\n",
      "Size: 86 articles\n",
      "Date range: 2024-10-25T00:00:00 to 2024-11-02T00:00:00\n",
      "Sample titles:\n",
      "- UKCV: Nema novih žrtava, troje povređenih i dalje u teškom stanju\n",
      "- Arhitekta: Urušavanje dela Železničke stanice znak nebrige i nemara prilikom gradnje\n",
      "- Zvaničnici EU i više država izrazili saučešće građanima Srbije povodom nesreće u Novom Sadu\n",
      "- Pašalić: U najkraćem roku utvrditi odgovornost za tragediju u Novom Sadu\n",
      "- Direktorka KC Vojvodine: Tri osobe u veoma teškom stanju, sve su životno ugrožene\n",
      "\n",
      "Representative documents for Cluster 0:\n",
      "- Miketić poziva na „blokadu upozorenja“ Beograda na vodi – borba za očuvanje Savskog mosta\n",
      "- Aktivisti najavili da ostaju na Starom savskom mostu kako bi sprečili rušenje\n",
      "- Špic tokom celog dana, ali semafor ima radno vreme: Vikendom odmara\n",
      "\n",
      "Representative documents for Cluster 1:\n",
      "- Fon der Lajen u razgovoru s Vučićem: EU će poštovati i čuvati divnu prirodu Srbije\n",
      "- Evroposlanik o litijumu: EU pravi grešku – zbog gladi oko resursa podržava nedemokratske režime\n",
      "- Vučeviću uručen izveštaj EK o Srbiji: Shvatili smo poruke Brisela koje ne čujemo prvi put\n",
      "\n",
      "Representative documents for Cluster 2:\n",
      "- Tužilaštvo predložilo pritvor za muškaraca koji je pucao u ženu i sina na Bežanijskoj kosi\n",
      "- Istraga protiv osumnjičenog da je obljubio devojku\n",
      "- Uhapšene dve osobe: Osumnjičeni da nisu uplaćivali pazar, nego novac zadržavali\n",
      "\n",
      "Representative documents for Cluster 3:\n",
      "- Vlada Srbije usvojila više finansijskih zakona: Od poreza do fondova\n",
      "- Stamenkovski: Od 1. novembra na snazi izmene Zakona o finansijskoj podršci porodicama s decom\n",
      "- Pritisak na prevoznike iz Niša da ne dovoze prosvetare na protest: „Upozorenje iz BIA i MUP“\n",
      "\n",
      "Representative documents for Cluster 4:\n",
      "- Filipović: Povezali smo sajt „Kopaćemo“ s državom i podatke dostavili sudu\n",
      "- Jovanović Ćuta u Loznici: Ovo je potencijalno mesto zločina, ne znamo šta je u glavi psihopate\n",
      "- Na skupštini Loznice nepoželjna opozicija, a živi zid i barikade i za aktiviste i poslanike\n",
      "\n",
      "Representative documents for Cluster 5:\n",
      "- Završena akcija spasavanja: 14 stradalih i troje teško povređenih u urušavanju nadstrešnice na Železničkoj stanici\n",
      "- U Novom Sadu počinje trodnevna žalost, okupljanje u 17 časova\n",
      "- Dačić: Osmoro mrtvih u nesreći u Novom Sadu, ispod ruševina devojka\n"
     ]
    }
   ],
   "source": [
    "# Get MongoDB connection using your existing function\n",
    "client, db, collection = get_mongodb_connection()\n",
    "\n",
    "# Run the analysis\n",
    "results = analyze_article_topics(\n",
    "    collection=collection,\n",
    "    min_k=2,\n",
    "    max_k=10\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Optimal number of clusters: {results['optimal_k']}\")\n",
    "\n",
    "# Print cluster statistics\n",
    "for stats in results['cluster_stats']:\n",
    "    print(f\"\\nCluster {stats['cluster_id']}:\")\n",
    "    print(f\"Size: {stats['size']} articles\")\n",
    "    print(f\"Date range: {stats['earliest_date']} to {stats['latest_date']}\")\n",
    "    print(\"Sample titles:\")\n",
    "    for title in stats['sample_titles']:\n",
    "        print(f\"- {title}\")\n",
    "\n",
    "# Print representative documents\n",
    "for cluster_id, docs in results['representative_documents'].items():\n",
    "    print(f\"\\nRepresentative documents for Cluster {cluster_id}:\")\n",
    "    for doc in docs:\n",
    "        print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming the clusters with Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude client\n",
    "anthropic = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Cluster Names:\n",
      "==================================================\n",
      "\n",
      "Cluster 0: Bridge Protests\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Miketić poziva na „blokadu upozorenja“ Beograda na vodi – borba za očuvanje Savskog mosta\n",
      "- Aktivisti najavili da ostaju na Starom savskom mostu kako bi sprečili rušenje\n",
      "\n",
      "Cluster 1: EU-Serbia Relations\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Fon der Lajen u razgovoru s Vučićem: EU će poštovati i čuvati divnu prirodu Srbije\n",
      "- Evroposlanik o litijumu: EU pravi grešku – zbog gladi oko resursa podržava nedemokratske režime\n",
      "\n",
      "Cluster 2: Criminal Investigation\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Tužilaštvo predložilo pritvor za muškaraca koji je pucao u ženu i sina na Bežanijskoj kosi\n",
      "- Istraga protiv osumnjičenog da je obljubio devojku\n",
      "\n",
      "Cluster 3: Government Pressure\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Vlada Srbije usvojila više finansijskih zakona: Od poreza do fondova\n",
      "- Stamenkovski: Od 1. novembra na snazi izmene Zakona o finansijskoj podršci porodicama s decom\n",
      "\n",
      "Cluster 4: Environmental Activism\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Filipović: Povezali smo sajt „Kopaćemo“ s državom i podatke dostavili sudu\n",
      "- Jovanović Ćuta u Loznici: Ovo je potencijalno mesto zločina, ne znamo šta je u glavi psihopate\n",
      "\n",
      "Cluster 5: Tragic Building Collapse\n",
      "------------------------------\n",
      "Sample titles:\n",
      "- Završena akcija spasavanja: 14 stradalih i troje teško povređenih u urušavanju nadstrešnice na Železničkoj stanici\n",
      "- U Novom Sadu počinje trodnevna žalost, okupljanje u 17 časova\n"
     ]
    }
   ],
   "source": [
    "def generate_cluster_names(\n",
    "    results: Dict,\n",
    "    anthropic_client: Anthropic,\n",
    "    use_excerpts: bool = False,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 2\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Generate descriptive names for clusters using Claude API.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing clustering results\n",
    "        anthropic_client: Initialized Anthropic client\n",
    "        use_excerpts: If True, use document excerpts instead of just titles\n",
    "        max_retries: Maximum number of retries for API calls\n",
    "        retry_delay: Delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster IDs to generated names\n",
    "    \"\"\"\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id, docs in results['representative_documents'].items():\n",
    "        # Prepare the content for Claude\n",
    "        if use_excerpts:\n",
    "            content = \"\\n\".join([\n",
    "                f\"Document {i+1}:\\nTitle: {doc['title']}\\nExcerpt: {doc['preview']}\"\n",
    "                for i, doc in enumerate(docs)\n",
    "            ])\n",
    "        else:\n",
    "            content = \"\\n\".join([\n",
    "                f\"- {doc['title']}\" for doc in docs\n",
    "            ])\n",
    "        \n",
    "        # Prepare the prompt with stronger emphasis on English output\n",
    "        prompt = f\"\"\"You are an international news categorization expert. The documents below are Serbian news articles.\n",
    "Your task is to provide a short (2-4 words) ENGLISH LANGUAGE descriptive name for this thematic cluster.\n",
    "\n",
    "For example:\n",
    "- If articles are about \"Finansijski zakoni\", name it \"Financial Legislation\"\n",
    "- If articles are about \"Ekološki protesti\", name it \"Environmental Protests\"\n",
    "- If articles are about \"Politička kriza\", name it \"Political Crisis\"\n",
    "\n",
    "Documents from cluster:\n",
    "{content}\n",
    "\n",
    "IMPORTANT: Respond ONLY with the English language cluster name, no Serbian words allowed.\n",
    "Example good responses: \"Economic Reform\", \"Infrastructure Development\", \"Criminal Investigation\"\n",
    "Example bad responses: \"Finansijski zakoni\", \"Ekološki protesti\", \"Politička kriza\" \"\"\"\n",
    "\n",
    "        # Try to get response with retries\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=\"claude-3-opus-20240229\",\n",
    "                    max_tokens=30,\n",
    "                    temperature=0.2,\n",
    "                    messages=[{\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }]\n",
    "                )\n",
    "                \n",
    "                cluster_name = response.content[0].text.strip()\n",
    "                # Additional check to ensure the response is in English\n",
    "                if any(c.lower() in cluster_name.lower() for c in ['č', 'ć', 'š', 'ž', 'đ']):\n",
    "                    raise ValueError(\"Response contains Serbian characters\")\n",
    "                cluster_names[cluster_id] = cluster_name\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Failed to get name for cluster {cluster_id}: {str(e)}\")\n",
    "                    cluster_names[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "                else:\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "    \n",
    "    return cluster_names\n",
    "\n",
    "# Use the function\n",
    "try:\n",
    "    cluster_names = generate_cluster_names(results, anthropic)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGenerated Cluster Names:\")\n",
    "    print(\"=\" * 50)\n",
    "    for cluster_id, name in cluster_names.items():\n",
    "        print(f\"\\nCluster {cluster_id}: {name}\")\n",
    "        print(\"-\" * 30)\n",
    "        # Print a few sample titles for reference\n",
    "        print(\"Sample titles:\")\n",
    "        for doc in results['representative_documents'][cluster_id][:2]:\n",
    "            print(f\"- {doc['title']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating cluster names: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Bridge Protests',\n",
       " 1: 'EU-Serbia Relations',\n",
       " 2: 'Criminal Investigation',\n",
       " 3: 'Government Pressure',\n",
       " 4: 'Environmental Activism',\n",
       " 5: 'Tragic Building Collapse'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summaries in English with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_news_report(\n",
    "    query: str,\n",
    "    collection,\n",
    "    model: SentenceTransformer,\n",
    "    anthropic_client: Anthropic,\n",
    "    language: str = \"English\",\n",
    "    top_k: int = 10,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform semantic search and generate a news report based on top results.\n",
    "    Translates the query to Serbian before searching and generates report in specified language.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query in English\n",
    "        collection: MongoDB collection\n",
    "        model: Sentence transformer model for embeddings\n",
    "        anthropic_client: Anthropic client\n",
    "        language: Output language for the report\n",
    "        top_k: Number of top articles to consider\n",
    "        max_retries: Maximum number of API retries\n",
    "        retry_delay: Delay between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Generated news report\n",
    "    \"\"\"\n",
    "    # Translate query to Serbian using Claude\n",
    "    translation_prompt = f\"Translate the following {language} text to Serbian latin. Provide only the translation, nothing else: '{query}'\"\n",
    "    \n",
    "    try:\n",
    "        translation_response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=100,\n",
    "            temperature=0,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": translation_prompt\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        serbian_query = translation_response.content[0].text.strip()\n",
    "        print(serbian_query)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Translation failed: {str(e)}\")\n",
    "    \n",
    "    # Generate embedding for Serbian query\n",
    "    query_embedding = model.encode(serbian_query).tolist()\n",
    "    \n",
    "    # Perform vector search\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"title\": 1,\n",
    "                \"content\": 1,\n",
    "                \"date\": 1,\n",
    "                \"score\": { \"$meta\": \"vectorSearchScore\" }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    \n",
    "    # Prepare content for Claude\n",
    "    articles_text = \"\\n\\n\".join([\n",
    "        f\"Article {i+1}:\\nDate: {doc['date']}\\nTitle: {doc['title']}\\nContent: {doc['content'][:500]}...\"\n",
    "        for i, doc in enumerate(results)\n",
    "    ])\n",
    "    \n",
    "    # Create prompt for report generation\n",
    "    prompt = f\"\"\"You are an expert journalist and news analyst. Based on the following {top_k} most relevant Serbian news articles about \"{query}\" (translated to Serbian as \"{serbian_query}\"), \n",
    "create a concise, well-structured news report in {language}. The report should:\n",
    "- Be around 250-300 words\n",
    "- Start with a clear headline\n",
    "- Include key facts, dates, and relevant context\n",
    "- Maintain journalistic neutrality\n",
    "- Focus on the most newsworthy aspects\n",
    "- Include a brief conclusion or outlook\n",
    "Here are the articles:\n",
    "{articles_text}\n",
    "Please write the report in a professional journalistic style.\"\"\"\n",
    "\n",
    "    # Get response from Claude with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = anthropic_client.messages.create(\n",
    "                model=\"claude-3-opus-20240229\",\n",
    "                max_tokens=1000,\n",
    "                temperature=0.3,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            report = response.content[0].text.strip()\n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise Exception(f\"Failed to generate report: {str(e)}\")\n",
    "            time.sleep(retry_delay)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investicije u rudarstvo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5661710edb4fce8b29e7a9f68db159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated News Report:\n",
      "================================================================================\n",
      "Headline: Serbia Grapples with Mining Investments Amidst Environmental Concerns and Political Debates\n",
      "\n",
      "In recent weeks, the topic of mining investments in Serbia has been at the forefront of public discourse, with a particular focus on the potential lithium mine in the Jadar Valley. The Serbian government and mining giant Rio Tinto have been pushing for the project, while environmental activists and some opposition politicians have voiced their concerns.\n",
      "\n",
      "On October 25, 2024, President Aleksandar Vučić expressed hope that Serbia would have a stake in the potential lithium mine, hinting at a possible partnership with Rio Tinto. However, the company has remained silent on the matter, despite actively responding to other aspects of the Jadar project.\n",
      "\n",
      "Meanwhile, the Ministry of Mining and Energy has allocated 120 million dinars (approximately €1 million) to encourage local governments to install solar panels on public buildings. The move is seen as a step towards diversifying Serbia's energy sources and promoting renewable energy.\n",
      "\n",
      "Opposition figures, such as Miroslav Aleksić from the People's Movement of Serbia, have accused the government of turning Serbia into a \"mining colony\" while failing to address corruption. Activists from the \"Ne damo Jadar\" (We Won't Give Jadar) association have also criticized former minister Zorana Mihajlović for her support of the lithium mine project.\n",
      "\n",
      "On the economic front, the state-owned power utility Elektroprivreda Srbije (EPS) reported a profit of nearly 30 billion dinars (€254 million) in the first nine months of 2024, surpassing its planned profit by 30%. However, some opposition politicians have questioned the company's performance, citing production issues and increased electricity imports.\n",
      "\n",
      "As Serbia navigates the complex landscape of mining investments and environmental concerns, the future of the Jadar lithium mine project remains uncertain. The government's stance on foreign investments and its ability to balance economic growth with environmental protection will likely shape the country's mining sector in the years to come.\n",
      "\n",
      "Word count: 299\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "try:\n",
    "    # Example query\n",
    "    query = \"Investments in mining\"\n",
    "\n",
    "    report = generate_news_report(\n",
    "        query=query,\n",
    "        collection=collection,\n",
    "        model=model,\n",
    "        anthropic_client=anthropic,\n",
    "        top_k=10,\n",
    "        language=\"English\"\n",
    "\n",
    "    )\n",
    "\n",
    "    print(\"\\nGenerated News Report:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating report: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
